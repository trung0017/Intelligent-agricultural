from fastapi import FastAPI, Request, Form, HTTPException, Depends, UploadFile, File, BackgroundTasks
from fastapi.responses import HTMLResponse, RedirectResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import os
import markdown
from pathlib import Path
import json
from datetime import datetime
import aiofiles
import asyncio
import subprocess
import threading
import time
from typing import List, Dict, Any
import uuid

app = FastAPI(title="WikiNongSan")

# C·∫•u h√¨nh th∆∞ m·ª•c
PAGES_DIR = Path("pages")
STATIC_DIR = Path("static")
TEMPLATES_DIR = Path("templates")

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
PAGES_DIR.mkdir(exist_ok=True)
STATIC_DIR.mkdir(exist_ok=True)
TEMPLATES_DIR.mkdir(exist_ok=True)

app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")
templates = Jinja2Templates(directory=TEMPLATES_DIR)

# C·∫•u h√¨nh admin ƒë∆°n gi·∫£n (trong th·ª±c t·∫ø n√™n d√πng database)
ADMIN_USERNAME = "admin"
ADMIN_PASSWORD = "123"

# L∆∞u tr·ªØ tr·∫°ng th√°i c√°c task ƒëang ch·∫°y
running_tasks = {}
task_logs = {}

def check_admin(username: str = Form(), password: str = Form()):
    if username != ADMIN_USERNAME or password != ADMIN_PASSWORD:
        raise HTTPException(status_code=401, detail="Sai t√™n ƒëƒÉng nh·∫≠p ho·∫∑c m·∫≠t kh·∫©u")
    return True

def get_all_pages():
    """L·∫•y danh s√°ch t·∫•t c·∫£ c√°c trang"""
    pages = []
    for file_path in PAGES_DIR.glob("*.md"):
        # ƒê·ªçc ti√™u ƒë·ªÅ t·ª´ n·ªôi dung file (d√≤ng ƒë·∫ßu ti√™n b·∫Øt ƒë·∫ßu v·ªõi #)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # T√¨m d√≤ng ƒë·∫ßu ti√™n b·∫Øt ƒë·∫ßu v·ªõi #
                for line in content.split('\n'):
                    if line.strip().startswith('# '):
                        title = line.strip()[2:].strip()  # B·ªè "# " ·ªü ƒë·∫ßu
                        break
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y, d√πng t√™n file nh∆∞ng b·ªè timestamp
                    title = file_path.stem
                    # B·ªè timestamp (ph·∫ßn _s·ªë cu·ªëi)
                    if '_' in title:
                        parts = title.split('_')
                        if parts[-1].isdigit():  # N·∫øu ph·∫ßn cu·ªëi l√† s·ªë (timestamp)
                            title = '_'.join(parts[:-1])
                    title = title.replace("_", " ").title()
        except:
            # Fallback n·∫øu c√≥ l·ªói ƒë·ªçc file
            title = file_path.stem.replace("_", " ").title()
        
        pages.append({
            "filename": file_path.name,
            "title": title,
            "slug": file_path.stem
        })
    return sorted(pages, key=lambda x: x["title"])

def markdown_to_html(content: str) -> str:
    """Chuy·ªÉn ƒë·ªïi Markdown sang HTML"""
    md = markdown.Markdown(extensions=['extra', 'codehilite'])
    return md.convert(content)

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """Trang ch·ªß hi·ªÉn th·ªã danh s√°ch t·∫•t c·∫£ c√°c trang"""
    pages = get_all_pages()
    return templates.TemplateResponse("home.html", {
        "request": request,
        "pages": pages
    })

@app.get("/page/{slug}", response_class=HTMLResponse)
async def view_page(request: Request, slug: str):
    """Xem n·ªôi dung m·ªôt trang"""
    file_path = PAGES_DIR / f"{slug}.md"
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y trang")
    
    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
        content = await f.read()
    
    html_content = markdown_to_html(content)
    
    # ƒê·ªçc ti√™u ƒë·ªÅ t·ª´ n·ªôi dung file (d√≤ng ƒë·∫ßu ti√™n b·∫Øt ƒë·∫ßu v·ªõi #)
    title = None
    for line in content.split('\n'):
        if line.strip().startswith('# '):
            title = line.strip()[2:].strip()  # B·ªè "# " ·ªü ƒë·∫ßu
            break
    
    # N·∫øu kh√¥ng t√¨m th·∫•y ti√™u ƒë·ªÅ trong n·ªôi dung, d√πng slug nh∆∞ng b·ªè timestamp
    if not title:
        title = slug
        # B·ªè timestamp (ph·∫ßn _s·ªë cu·ªëi)
        if '_' in title:
            parts = title.split('_')
            if parts[-1].isdigit():  # N·∫øu ph·∫ßn cu·ªëi l√† s·ªë (timestamp)
                title = '_'.join(parts[:-1])
        title = title.replace("_", " ").title()
    
    return templates.TemplateResponse("page.html", {
        "request": request,
        "title": title,
        "content": html_content,
        "slug": slug
    })

@app.get("/admin", response_class=HTMLResponse)
async def admin_login(request: Request):
    """Trang ƒëƒÉng nh·∫≠p admin"""
    return templates.TemplateResponse("admin_login.html", {"request": request})

@app.post("/admin/login")
async def admin_login_post(admin_check: bool = Depends(check_admin)):
    """X·ª≠ l√Ω ƒëƒÉng nh·∫≠p admin"""
    return RedirectResponse(url="/admin/dashboard", status_code=302)

@app.get("/admin/dashboard", response_class=HTMLResponse)
async def admin_dashboard(request: Request):
    """Dashboard admin"""
    pages = get_all_pages()
    return templates.TemplateResponse("admin_dashboard.html", {
        "request": request,
        "pages": pages
    })

@app.get("/admin/crawler", response_class=HTMLResponse)
async def admin_crawler(request: Request):
    """Trang crawler v√† AI processing"""
    return templates.TemplateResponse("admin_crawler.html", {
        "request": request
    })

@app.get("/admin/create", response_class=HTMLResponse)
async def admin_create_page(request: Request):
    """Form t·∫°o trang m·ªõi"""
    return templates.TemplateResponse("admin_create.html", {"request": request})

@app.post("/admin/create")
async def admin_create_page_post(
    title: str = Form(),
    content: str = Form()
):
    """T·∫°o trang m·ªõi"""
    slug = title.lower().replace(" ", "_").replace("-", "_")
    file_path = PAGES_DIR / f"{slug}.md"
    
    # Th√™m metadata
    full_content = f"""# {title}

{content}

---
*T·∫°o ng√†y: {datetime.now().strftime('%d/%m/%Y %H:%M')}*
"""
    
    async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
        await f.write(full_content)
    
    return RedirectResponse(url=f"/page/{slug}", status_code=302)

@app.get("/admin/edit/{slug}", response_class=HTMLResponse)
async def admin_edit_page(request: Request, slug: str):
    """Form ch·ªânh s·ª≠a trang"""
    file_path = PAGES_DIR / f"{slug}.md"
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y trang")
    
    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
        content = await f.read()
    
    title = slug.replace("_", " ").title()
    return templates.TemplateResponse("admin_edit.html", {
        "request": request,
        "title": title,
        "content": content,
        "slug": slug
    })

@app.post("/admin/edit/{slug}")
async def admin_edit_page_post(
    slug: str,
    content: str = Form()
):
    """C·∫≠p nh·∫≠t trang"""
    file_path = PAGES_DIR / f"{slug}.md"
    
    async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
        await f.write(content)
    
    return RedirectResponse(url=f"/page/{slug}", status_code=302)

@app.post("/admin/delete/{slug}")
async def admin_delete_page(slug: str):
    """X√≥a trang"""
    file_path = PAGES_DIR / f"{slug}.md"
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Trang kh√¥ng t·ªìn t·∫°i")
    
    try:
        file_path.unlink()  # X√≥a file
        return JSONResponse(content={
            "success": True,
            "message": f"ƒê√£ x√≥a '{slug}'"
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": f"L·ªói khi x√≥a trang: {str(e)}"
            }
        )

@app.post("/admin/delete-all")
async def admin_delete_all_pages():
    """X√≥a t·∫•t c·∫£ trang"""
    try:
        deleted_count = 0
        
        if PAGES_DIR.exists():
            for file_path in PAGES_DIR.glob("*.md"):
                file_path.unlink()
                deleted_count += 1
        
        return JSONResponse(content={
            "success": True,
            "message": f"ƒê√£ x√≥a {deleted_count} trang",
            "deleted_count": deleted_count
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": f"L·ªói khi x√≥a t·∫•t c·∫£: {str(e)}"
            }
        )

@app.post("/admin/api/upload-image")
async def admin_upload_image(file: UploadFile = File(...), slug: str = Form(...)):
    """Upload ·∫£nh cho b√†i vi·∫øt c·ª• th·ªÉ"""

    try:
        # Ki·ªÉm tra file ·∫£nh
        if not file.content_type.startswith('image/'):
            return JSONResponse(
                status_code=400,
                content={"error": "Ch·ªâ ch·∫•p nh·∫≠n file ·∫£nh"}
            )
        
        # T·∫°o th∆∞ m·ª•c uploads n·∫øu ch∆∞a c√≥
        uploads_dir = Path("static/uploads")
        uploads_dir.mkdir(exist_ok=True)
        
        # L∆∞u file v·ªõi t√™n theo slug
        image_path = uploads_dir / f"{slug}.jpg"
        
        with open(image_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        return JSONResponse(content={
            "success": True,
            "message": "Upload ·∫£nh th√†nh c√¥ng",
            "image_url": f"/static/uploads/{slug}.jpg"
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"L·ªói upload: {str(e)}"}
        )

@app.post("/admin/api/delete-image")
async def admin_delete_image(slug: str = Form(...)):
    """X√≥a ·∫£nh c·ªßa b√†i vi·∫øt c·ª• th·ªÉ"""
    try:
        image_path = Path(f"static/uploads/{slug}.jpg")
        
        if image_path.exists():
            image_path.unlink()
            return JSONResponse(content={
                "success": True,
                "message": "ƒê√£ x√≥a ·∫£nh b√†i vi·∫øt"
            })
        else:
            return JSONResponse(content={
                "success": False,
                "message": "Kh√¥ng c√≥ ·∫£nh ƒë·ªÉ x√≥a"
            })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"L·ªói x√≥a ·∫£nh: {str(e)}"}
        )

@app.post("/admin/upload")
async def admin_upload_file(file: UploadFile = File(...)):
    """Upload file ƒë√£ l√†m s·∫°ch"""
    if not file.filename.endswith('.md'):
        raise HTTPException(status_code=400, detail="Ch·ªâ ch·∫•p nh·∫≠n file .md")
    
    content = await file.read()
    file_path = PAGES_DIR / file.filename
    
    async with aiofiles.open(file_path, 'wb') as f:
        await f.write(content)
    
    return {"message": f"ƒê√£ upload th√†nh c√¥ng {file.filename}"}

# ===== CRAWLER & AI PROCESSING APIs =====

def run_crawler_task(task_id: str, urls: List[str], topic: str):
    """Ch·∫°y crawler trong background thread"""
    try:
        task_logs[task_id] = []
        
        def log_message(msg: str):
            task_logs[task_id].append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        
        log_message("üöÄ B·∫Øt ƒë·∫ßu thu th·∫≠p n·ªôi dung...")
        
        # Import crawler modules
        from crawl import WebCrawler
        
        crawler = WebCrawler()
        
        # Crawl t·ª´ng URL
        crawled_data = []
        for i, url in enumerate(urls, 1):
            log_message(f"üì° [{i}/{len(urls)}] ƒêang crawl: {url}")
            
            try:
                result = crawler.crawl_url(url, use_playwright=False)
                
                if result and len(result['content']) < 500:
                    log_message("üìÑ N·ªôi dung ng·∫Øn, th·ª≠ l·∫°i v·ªõi Playwright...")
                    result = crawler.crawl_url(url, use_playwright=True)
                
                if result:
                    crawled_data.append(result)
                    log_message(f"‚úÖ Th√†nh c√¥ng: {result['title'][:50]}... ({len(result['content'])} k√Ω t·ª±)")
                    
                    # L∆∞u raw file
                    raw_file = crawler.save_raw_content(result)
                    log_message(f"üíæ ƒê√£ l∆∞u: {raw_file}")
                else:
                    log_message(f"‚ùå Kh√¥ng th·ªÉ crawl {url}")
                    
            except Exception as e:
                log_message(f"‚ùå L·ªói crawl {url}: {str(e)}")
            
            time.sleep(2)  # Ngh·ªâ gi·ªØa c√°c l·∫ßn crawl
        
        if not crawled_data:
            log_message("‚ùå Kh√¥ng crawl ƒë∆∞·ª£c trang n√†o!")
            running_tasks[task_id] = "failed"
            return
        
        log_message(f"üìù ƒê√£ crawl th√†nh c√¥ng {len(crawled_data)}/{len(urls)} trang")
        
        # T·ªïng h·ª£p n·ªôi dung
        log_message("üîÑ ƒêang t·ªïng h·ª£p n·ªôi dung...")
        
        combined_content = ""
        sources = []
        
        for i, data in enumerate(crawled_data, 1):
            combined_content += f"\n\n## Ngu·ªìn {i}: {data['title']}\n\n"
            combined_content += data['content'][:2000]
            if len(data['content']) > 2000:
                combined_content += "\n\n[...n·ªôi dung ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn...]"
            
            sources.append({
                'title': data['title'],
                'url': data['url']
            })
        
        # L∆∞u th√¥ng tin t·ªïng h·ª£p ƒë·ªÉ x·ª≠ l√Ω sau
        summary_data = {
            "topic": topic,
            "crawled_count": len(crawled_data),
            "total_urls": len(urls),
            "sources": sources,
            "timestamp": time.strftime('%d/%m/%Y %H:%M'),
            "combined_content": combined_content
        }
        
        # L∆∞u summary file
        summary_file = Path("raw_content") / f"summary_{topic.replace(' ', '_')}_{int(time.time())}.json"
        summary_file.parent.mkdir(exist_ok=True)
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        log_message(f"üìã ƒê√£ l∆∞u th√¥ng tin t·ªïng h·ª£p: {summary_file}")
        log_message("‚úÖ Thu th·∫≠p ho√†n t·∫•t!")
        log_message("üí° S·ª≠ d·ª•ng 'Text Cleaner' ƒë·ªÉ x·ª≠ l√Ω AI v√† t·∫°o wiki")
        
        running_tasks[task_id] = "completed"
        
    except Exception as e:
        task_logs[task_id].append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå L·ªói: {str(e)}")
        running_tasks[task_id] = "failed"

@app.post("/admin/api/crawler/start")
async def start_crawler_task(
    background_tasks: BackgroundTasks,
    urls: str = Form(),
    topic: str = Form()
):
    """B·∫Øt ƒë·∫ßu task crawler"""
    try:
        # Parse URLs
        url_list = [url.strip() for url in urls.split('\n') if url.strip()]
        
        if len(url_list) < 1:
            return JSONResponse(
                status_code=400,
                content={"error": "C·∫ßn √≠t nh·∫•t 1 URL ƒë·ªÉ thu th·∫≠p"}
            )
        
        if len(url_list) > 5:
            return JSONResponse(
                status_code=400,
                content={"error": "T·ªëi ƒëa 5 URL m·ªói l·∫ßn"}
            )
        
        # Validate URLs
        for url in url_list:
            if not (url.startswith('http://') or url.startswith('https://')):
                return JSONResponse(
                    status_code=400,
                    content={"error": f"URL kh√¥ng h·ª£p l·ªá: {url}"}
                )
        
        # T·∫°o task ID
        task_id = str(uuid.uuid4())
        running_tasks[task_id] = "running"
        
        # Ch·∫°y trong background thread
        thread = threading.Thread(
            target=run_crawler_task,
            args=(task_id, url_list, topic)
        )
        thread.daemon = True
        thread.start()
        
        return JSONResponse(content={
            "task_id": task_id,
            "message": "ƒê√£ b·∫Øt ƒë·∫ßu thu th·∫≠p n·ªôi dung",
            "urls_count": len(url_list)
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"L·ªói kh·ªüi t·∫°o task: {str(e)}"}
        )

@app.get("/admin/api/crawler/status/{task_id}")
async def get_crawler_status(task_id: str):
    """L·∫•y tr·∫°ng th√°i task crawler"""
    if task_id not in running_tasks:
        return JSONResponse(
            status_code=404,
            content={"error": "Task kh√¥ng t·ªìn t·∫°i"}
        )
    
    return JSONResponse(content={
        "task_id": task_id,
        "status": running_tasks[task_id],
        "logs": task_logs.get(task_id, [])
    })

@app.get("/admin/api/crawler/logs/{task_id}")
async def get_crawler_logs(task_id: str):
    """L·∫•y logs c·ªßa task crawler"""
    if task_id not in task_logs:
        return JSONResponse(content={"logs": []})
    
    return JSONResponse(content={
        "task_id": task_id,
        "logs": task_logs[task_id]
    })

@app.post("/admin/api/clean-text")
async def clean_text_api(
    background_tasks: BackgroundTasks,
    action: str = Form(),  # "single" ho·∫∑c "batch"
    custom_prompt: str = Form(None)  # Prompt t√πy ch·ªânh t·ª´ admin
):
    """API l√†m s·∫°ch vƒÉn b·∫£n"""
    try:
        from clean_text import TextCleaner
        
        cleaner = TextCleaner()
        
        if action == "batch":
            # X·ª≠ l√Ω h√†ng lo·∫°t - t√¨m summary files v√† t·∫°o wiki
            raw_dir = Path("raw_content")
            if not raw_dir.exists():
                return JSONResponse(
                    status_code=400,
                    content={"error": "Th∆∞ m·ª•c raw_content kh√¥ng t·ªìn t·∫°i"}
                )
            
            # T√¨m summary files
            summary_files = list(raw_dir.glob("summary_*.json"))
            regular_files = [f for f in raw_dir.glob("*.json") if not f.name.startswith("summary_")]
            
            # Ki·ªÉm tra c√≥ file summary ƒë·ªÉ x·ª≠ l√Ω kh√¥ng
            if not summary_files:
                return JSONResponse(
                    status_code=400,
                    content={
                        "error": "Kh√¥ng c√≥ file summary ƒë·ªÉ t·∫°o wiki",
                        "message": "C·∫ßn file summary t·ª´ Multi-Source Crawler ƒë·ªÉ t·∫°o b√†i wiki.",
                        "suggestion": "S·ª≠ d·ª•ng Multi-Source Crawler ƒë·ªÉ t·∫°o file summary",
                        "note": f"C√≥ {len(regular_files)} file JSON th√¥ng th∆∞·ªùng nh∆∞ng ch·ªâ x·ª≠ l√Ω file summary"
                    }
                )
            
            results = []
            
            # X·ª≠ l√Ω summary files (t·∫°o wiki t·ª´ crawler data)
            for summary_file in summary_files:
                try:
                    with open(summary_file, 'r', encoding='utf-8') as f:
                        summary_data = json.load(f)
                    
                    topic = summary_data['topic']
                    combined_content = summary_data['combined_content']
                    sources = summary_data['sources']
                    
                    # AI processing
                    ai_available = cleaner.test_ollama_connection()
                    
                    if ai_available:
                        # S·ª≠ d·ª•ng custom prompt n·∫øu c√≥, n·∫øu kh√¥ng d√πng m·∫∑c ƒë·ªãnh
                        if custom_prompt and custom_prompt.strip():
                            # Thay th·∫ø placeholder trong custom prompt
                            synthesis_prompt = custom_prompt.replace('{content}', combined_content[:4000])
                            synthesis_prompt = synthesis_prompt.replace('{topic}', topic)
                        else:
                            # Prompt m·∫∑c ƒë·ªãnh
                            synthesis_prompt = f"""
B·∫°n l√† chuy√™n gia vi·∫øt b√†i v·ªÅ n√¥ng nghi·ªáp. H√£y t·ªïng h·ª£p v√† vi·∫øt l·∫°i n·ªôi dung sau th√†nh m·ªôt b√†i vi·∫øt wiki ho√†n ch·ªânh v·ªÅ ch·ªß ƒë·ªÅ "{topic}".

Y√äU C·∫¶U:
1. T·∫°o m·ªôt b√†i vi·∫øt m·∫°ch l·∫°c, c√≥ c·∫•u tr√∫c r√µ r√†ng
2. Lo·∫°i b·ªè th√¥ng tin tr√πng l·∫∑p gi·ªØa c√°c ngu·ªìn
3. T·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn th√†nh n·ªôi dung th·ªëng nh·∫•t
4. S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng Markdown v·ªõi ti√™u ƒë·ªÅ, danh s√°ch, b·∫£ng bi·ªÉu
5. Gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng, lo·∫°i b·ªè qu·∫£ng c√°o
6. Vi·∫øt b·∫±ng ti·∫øng Vi·ªát, phong c√°ch wiki chuy√™n nghi·ªáp
7. Th√™m c√°c ph·∫ßn: Gi·ªõi thi·ªáu, N·ªôi dung ch√≠nh, K·∫øt lu·∫≠n

N·ªòI DUNG C·∫¶N T·ªîNG H·ª¢P:
{combined_content[:4000]}

B√ÄI VI·∫æT WIKI HO√ÄN CH·ªàNH:
"""
                        
                        try:
                            synthesized_content = cleaner.call_ollama(synthesis_prompt, max_tokens=3000)
                            if synthesized_content:
                                final_content = synthesized_content
                                method = "ai_synthesis"
                            else:
                                final_content = cleaner.clean_raw_text(combined_content)
                                method = "basic_synthesis"
                        except Exception:
                            final_content = cleaner.clean_raw_text(combined_content)
                            method = "basic_synthesis"
                    else:
                        final_content = cleaner.clean_raw_text(combined_content)
                        method = "basic_synthesis"
                    
                    # T·∫°o b√†i vi·∫øt wiki
                    sources_section = "\n## Ngu·ªìn tham kh·∫£o\n\n"
                    for i, source in enumerate(sources, 1):
                        sources_section += f"{i}. [{source['title']}]({source['url']})\n"
                    
                    wiki_content = f"""# {topic}

> **T√≥m t·∫Øt:** B√†i vi·∫øt t·ªïng h·ª£p t·ª´ {len(sources)} ngu·ªìn tin uy t√≠n v·ªÅ {topic.lower()}.

{final_content}

{sources_section}

---

**Ph∆∞∆°ng ph√°p:** {method}  
**S·ªë ngu·ªìn:** {len(sources)} trang web  
**Th·ªùi gian t·∫°o:** {summary_data['timestamp']}  
**C√¥ng c·ª•:** WikinongSang Web Crawler
"""
                    
                    # L∆∞u v√†o pages
                    safe_filename = topic.lower()
                    safe_filename = ''.join(c for c in safe_filename if c.isalnum() or c in (' ', '-', '_'))
                    safe_filename = safe_filename.replace(' ', '_')
                    safe_filename = f"{safe_filename}_{int(time.time())}.md"
                    
                    wiki_file = PAGES_DIR / safe_filename
                    
                    with open(wiki_file, 'w', encoding='utf-8') as f:
                        f.write(wiki_content)
                    
                    # T·∫°o URL slug s·∫°ch (kh√¥ng c√≥ timestamp)
                    clean_slug = topic.lower()
                    clean_slug = ''.join(c for c in clean_slug if c.isalnum() or c in (' ', '-', '_'))
                    clean_slug = clean_slug.replace(' ', '_')
                    
                    results.append({
                        "input": str(summary_file),
                        "output": f"Wiki: {wiki_file}",
                        "status": "success",
                        "type": "wiki_created",
                        "wiki_title": topic,
                        "wiki_url": f"/page/{safe_filename[:-3]}",  # URL v·ªõi timestamp (file th·∫≠t)
                        "wiki_display_title": topic,  # Ti√™u ƒë·ªÅ hi·ªÉn th·ªã g·ªëc
                        "sources_count": len(sources),
                        "method": method
                    })
                    
                    # X√≥a summary file sau khi x·ª≠ l√Ω
                    summary_file.unlink()
                    
                except Exception as e:
                    results.append({
                        "input": str(summary_file),
                        "error": str(e),
                        "status": "failed",
                        "type": "wiki_failed"
                    })
            
            # B·ªè qua regular files - ch·ªâ x·ª≠ l√Ω summary files ƒë·ªÉ t·∫°o wiki
            if regular_files:
                results.append({
                    "input": f"{len(regular_files)} file JSON th√¥ng th∆∞·ªùng",
                    "output": "B·ªè qua (ch·ªâ x·ª≠ l√Ω file summary)",
                    "status": "skipped",
                    "type": "skipped"
                })
            
            # T·∫°o th√¥ng b√°o t·ªïng k·∫øt
            wiki_created = [r for r in results if r["type"] == "wiki_created"]
            skipped_items = [r for r in results if r["type"] == "skipped"]
            failed_items = [r for r in results if r["status"] == "failed"]
            
            summary_message = []
            if wiki_created:
                summary_message.append(f"‚úÖ T·∫°o th√†nh c√¥ng {len(wiki_created)} b√†i wiki")
            if failed_items:
                summary_message.append(f"‚ùå Th·∫•t b·∫°i {len(failed_items)} file")
            
            return JSONResponse(content={
                "message": " | ".join(summary_message) if summary_message else "Ho√†n th√†nh x·ª≠ l√Ω",
                "results": results,
                "summary": {
                    "total_processed": len(wiki_created) + len(failed_items),
                    "wiki_created": len(wiki_created),
                    "skipped": len(skipped_items),
                    "failed": len(failed_items)
                }
            })
        
        else:
            return JSONResponse(
                status_code=400,
                content={"error": "Action kh√¥ng h·ª£p l·ªá"}
            )
            
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"L·ªói x·ª≠ l√Ω: {str(e)}"}
        )

@app.post("/admin/api/cleanup")
async def cleanup_files(
    target: str = Form()  # "raw_content", "cleaned_content", "all"
):
    """D·ªçn d·∫πp file"""
    try:
        deleted_count = 0
        
        if target == "raw_content" or target == "all":
            raw_dir = Path("raw_content")
            if raw_dir.exists():
                for file_path in raw_dir.glob("*.json"):
                    file_path.unlink()
                    deleted_count += 1
        
        if target == "cleaned_content" or target == "all":
            cleaned_dir = Path("cleaned_content")
            if cleaned_dir.exists():
                for file_path in cleaned_dir.glob("*.md"):
                    file_path.unlink()
                    deleted_count += 1
        
        return JSONResponse(content={
            "success": True,
            "message": f"ƒê√£ x√≥a {deleted_count} file t·ª´ {target}",
            "deleted_count": deleted_count
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": f"L·ªói khi d·ªçn d·∫πp: {str(e)}"
            }
        )

@app.get("/admin/api/files/{folder}")
async def get_file_list(folder: str):
    """L·∫•y danh s√°ch file trong th∆∞ m·ª•c"""
    try:
        if folder not in ['raw_content', 'cleaned_content', 'pages']:
            return JSONResponse(
                status_code=400,
                content={"error": "Th∆∞ m·ª•c kh√¥ng h·ª£p l·ªá"}
            )
        
        folder_path = Path(folder)
        if not folder_path.exists():
            return JSONResponse(content={"files": []})
        
        files = []
        for file_path in folder_path.iterdir():
            if file_path.is_file():
                stat = file_path.stat()
                files.append({
                    "name": file_path.name,
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).strftime('%d/%m/%Y %H:%M'),
                    "path": str(file_path)
                })
        
        # S·∫Øp x·∫øp theo th·ªùi gian s·ª≠a ƒë·ªïi m·ªõi nh·∫•t
        files.sort(key=lambda x: x['modified'], reverse=True)
        
        return JSONResponse(content={"files": files})
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"L·ªói ƒë·ªçc th∆∞ m·ª•c: {str(e)}"}
        )

@app.delete("/admin/api/files/{folder}/{filename}")
async def delete_single_file(folder: str, filename: str):
    """X√≥a m·ªôt file c·ª• th·ªÉ"""
    try:
        if folder not in ['raw_content', 'cleaned_content']:
            return JSONResponse(
                status_code=400,
                content={"error": "Kh√¥ng ƒë∆∞·ª£c ph√©p x√≥a file trong th∆∞ m·ª•c n√†y"}
            )
        
        file_path = Path(folder) / filename
        
        if not file_path.exists():
            return JSONResponse(
                status_code=404,
                content={"error": "File kh√¥ng t·ªìn t·∫°i"}
            )
        
        file_path.unlink()
        
        return JSONResponse(content={
            "success": True,
            "message": f"ƒê√£ x√≥a file {filename}"
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"L·ªói x√≥a file: {str(e)}"}
        )

@app.get("/admin/api/system/status")
async def get_system_status():
    """L·∫•y tr·∫°ng th√°i h·ªá th·ªëng"""
    try:
        from clean_text import TextCleaner
        
        # Ki·ªÉm tra Ollama (ch·ªâ ki·ªÉm tra nhanh, kh√¥ng g·ªçi AI)
        ollama_status = False
        ollama_error = ""
        try:
            import os
            import requests
            ollama_host = os.getenv('OLLAMA_HOST', 'localhost:11500')
            if not ollama_host.startswith('http'):
                ollama_host = f"http://{ollama_host}"
            
            response = requests.get(f"{ollama_host}/api/tags", timeout=5)
            ollama_status = response.status_code == 200
        except Exception as e:
            ollama_error = str(e)

            ollama_status = False
        
        # Ki·ªÉm tra th∆∞ m·ª•c
        directories = {
            "pages": PAGES_DIR.exists(),
            "raw_content": Path("raw_content").exists(),
            "cleaned_content": Path("cleaned_content").exists()
        }
        
        # ƒê·∫øm file
        file_counts = {
            "pages": len(list(PAGES_DIR.glob("*.md"))) if PAGES_DIR.exists() else 0,
            "raw_content": len(list(Path("raw_content").glob("*.json"))) if Path("raw_content").exists() else 0,
            "cleaned_content": len(list(Path("cleaned_content").glob("*.md"))) if Path("cleaned_content").exists() else 0
        }
        
        return JSONResponse(content={
            "ollama": {
                "status": "connected" if ollama_status else "disconnected",
                "url": ollama_host,
                "error": ollama_error if not ollama_status else None
            },
            "directories": directories,
            "file_counts": file_counts,
            "running_tasks": len([t for t in running_tasks.values() if t == "running"])
        })
        
    except Exception as e:
        return JSONResponse(content={
            "error": str(e),
            "ollama": {"status": "error"},
            "directories": {},
            "file_counts": {}
        })

@app.get("/search", response_class=HTMLResponse)
async def search_pages(request: Request, q: str = ""):
    """T√¨m ki·∫øm trang"""
    results = []
    if q:
        for file_path in PAGES_DIR.glob("*.md"):
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
            
            if q.lower() in content.lower() or q.lower() in file_path.stem.lower():
                title = file_path.stem.replace("_", " ").title()
                results.append({
                    "title": title,
                    "slug": file_path.stem,
                    "snippet": content[:200] + "..."
                })
    
    return templates.TemplateResponse("search.html", {
        "request": request,
        "query": q,
        "results": results
    })




if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)