#!/usr/bin/env python3
"""
WikinongSang Multi-Source Crawler
Thu tháº­p ná»™i dung tá»« nhiá»u trang web vÃ  tá»•ng há»£p thÃ nh bÃ i viáº¿t wiki hoÃ n chá»‰nh
"""

import os
import json
import time
from pathlib import Path
from crawl import WebCrawler
from clean_text import TextCleaner

def multi_source_crawler():
    """Thu tháº­p vÃ  tá»•ng há»£p ná»™i dung tá»« nhiá»u trang web"""
    
    print("ğŸŒ === WIKINONGSANG MULTI-SOURCE CRAWLER ===")
    print()
    
    # Khá»Ÿi táº¡o cÃ¡c cÃ´ng cá»¥
    crawler = WebCrawler()
    cleaner = TextCleaner()
    
    print("ğŸ”§ Kiá»ƒm tra há»‡ thá»‘ng...")
    print(f"ğŸ”— Ollama URL: {cleaner.ollama_url}")
    
    # Kiá»ƒm tra Ollama
    if cleaner.test_ollama_connection():
    
        ai_available = True
    else:
        print("âš ï¸ Ollama chÆ°a cháº¡y - sáº½ dÃ¹ng lÃ m sáº¡ch cÆ¡ báº£n")
        ai_available = False
    
    print()
    
    # Nháº­p URLs tá»« ngÆ°á»i dÃ¹ng
    print("ğŸ“ Nháº­p cÃ¡c URL Ä‘á»ƒ crawl (tá»‘i thiá»ƒu 2, tá»‘i Ä‘a 5 URL):")
    print("ğŸ’¡ Gá»£i Ã½: Chá»n cÃ¡c bÃ i viáº¿t cÃ¹ng chá»§ Ä‘á» Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t nháº¥t")
    print()
    
    urls = []
    
    while len(urls) < 5:
        url = input(f"URL {len(urls)+1} (Enter Ä‘á»ƒ káº¿t thÃºc náº¿u Ä‘Ã£ cÃ³ Ã­t nháº¥t 1 URL): ").strip()
        
        if not url:
            if len(urls) >= 1:
                break
            else:
                print("âŒ Cáº§n Ã­t nháº¥t 1 URL Ä‘á»ƒ thu tháº­p!")
                continue
        
        if url.startswith('http://') or url.startswith('https://'):
            urls.append(url)

        else:
            print("âŒ URL khÃ´ng há»£p lá»‡! Pháº£i báº¯t Ä‘áº§u báº±ng http:// hoáº·c https://")
    
    print(f"\nğŸ“‹ Sáº½ crawl {len(urls)} trang:")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    
    confirm = input("\nTiáº¿p tá»¥c? (y/n): ").lower()
    if confirm != 'y':
        print("âŒ ÄÃ£ há»§y!")
        return
    
    print()
    
    # BÆ°á»›c 1: Crawl ná»™i dung tháº­t
    print("ğŸ“¡ BÆ¯á»šC 1: Thu tháº­p ná»™i dung tá»« web")
    print("-" * 50)
    
    crawled_data = []
    
    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] Äang crawl: {url}")
        
        try:
            # Thá»­ crawl báº±ng requests trÆ°á»›c
            result = crawler.crawl_url(url, use_playwright=False)
            
            # Náº¿u ná»™i dung quÃ¡ ngáº¯n, thá»­ láº¡i vá»›i Playwright
            if result and len(result['content']) < 500:
                print("  ğŸ“„ Ná»™i dung ngáº¯n, thá»­ láº¡i vá»›i Playwright...")
                result = crawler.crawl_url(url, use_playwright=True)
            
            if result:
                crawled_data.append(result)
                print(f"  âœ… ThÃ nh cÃ´ng: {result['title'][:50]}...")

                
                # LÆ°u file thÃ´
                raw_file = crawler.save_raw_content(result)
                print(f"  ğŸ’¾ ÄÃ£ lÆ°u: {raw_file}")
            else:
                print(f"  âŒ KhÃ´ng thá»ƒ crawl {url}")
            
        except Exception as e:
            print(f"  âŒ Lá»—i crawl {url}: {e}")
        
        # Nghá»‰ giá»¯a cÃ¡c láº§n crawl
        if i < len(urls):
            print("  â³ Nghá»‰ 3 giÃ¢y...")
            time.sleep(3)
    
    if not crawled_data:
        print("\nâŒ KhÃ´ng crawl Ä‘Æ°á»£c trang nÃ o! Vui lÃ²ng kiá»ƒm tra URLs.")
        return
    
    print(f"\nâœ… ÄÃ£ crawl thÃ nh cÃ´ng {len(crawled_data)}/{len(urls)} trang")
    
    # BÆ°á»›c 2: Tá»•ng há»£p ná»™i dung
    print("\nğŸ”„ BÆ¯á»šC 2: Tá»•ng há»£p ná»™i dung")
    print("-" * 50)
    
    # Táº¡o tiÃªu Ä‘á» tá»•ng há»£p
    topic = input("Nháº­p chá»§ Ä‘á» chÃ­nh cho bÃ i viáº¿t tá»•ng há»£p: ").strip()
    if not topic:
        topic = "Tá»•ng há»£p thÃ´ng tin nÃ´ng nghiá»‡p"
    
    # Tá»•ng há»£p ná»™i dung
    combined_content = ""
    sources = []
    
    for i, data in enumerate(crawled_data, 1):
        combined_content += f"\n\n## Nguá»“n {i}: {data['title']}\n\n"
        combined_content += data['content'][:2000]  # Giá»›i háº¡n Ä‘á»™ dÃ i
        if len(data['content']) > 2000:
            combined_content += "\n\n[...ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c rÃºt gá»n...]"
        
        sources.append({
            'title': data['title'],
            'url': data['url']
        })
    
    print(f"ğŸ“ ÄÃ£ tá»•ng há»£p {len(crawled_data)} nguá»“n")
    print(f"ğŸ“Š Tá»•ng Ä‘á»™ dÃ i: {len(combined_content)} kÃ½ tá»±")
    
    # BÆ°á»›c 3: LÃ m sáº¡ch vÃ  tá»‘i Æ°u báº±ng AI
    print("\nğŸ¤– BÆ¯á»šC 3: LÃ m sáº¡ch vÃ  tá»‘i Æ°u ná»™i dung")
    print("-" * 50)
    
    if ai_available:
        print("ğŸ”„ Äang xá»­ lÃ½ báº±ng AI...")
        
        # Prompt Ä‘áº·c biá»‡t cho tá»•ng há»£p
        synthesis_prompt = f"""
Báº¡n lÃ  chuyÃªn gia viáº¿t bÃ i vá» nÃ´ng nghiá»‡p. HÃ£y tá»•ng há»£p vÃ  viáº¿t láº¡i ná»™i dung sau thÃ nh má»™t bÃ i viáº¿t wiki hoÃ n chá»‰nh vá» chá»§ Ä‘á» "{topic}".

YÃŠU Cáº¦U:
1. Táº¡o má»™t bÃ i viáº¿t máº¡ch láº¡c, cÃ³ cáº¥u trÃºc rÃµ rÃ ng
2. Loáº¡i bá» thÃ´ng tin trÃ¹ng láº·p giá»¯a cÃ¡c nguá»“n
3. Tá»•ng há»£p thÃ´ng tin tá»« nhiá»u nguá»“n thÃ nh ná»™i dung thá»‘ng nháº¥t
4. Sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng Markdown vá»›i tiÃªu Ä‘á», danh sÃ¡ch, báº£ng biá»ƒu
5. Giá»¯ láº¡i thÃ´ng tin quan trá»ng, loáº¡i bá» quáº£ng cÃ¡o
6. Viáº¿t báº±ng tiáº¿ng Viá»‡t, phong cÃ¡ch wiki chuyÃªn nghiá»‡p
7. ThÃªm cÃ¡c pháº§n: Giá»›i thiá»‡u, Ná»™i dung chÃ­nh, Káº¿t luáº­n

Ná»˜I DUNG Cáº¦N Tá»”NG Há»¢P:
{combined_content[:4000]}

BÃ€I VIáº¾T WIKI HOÃ€N CHá»ˆNH:
"""
        
        try:
            synthesized_content = cleaner.call_ollama(synthesis_prompt, max_tokens=3000)
            
            if synthesized_content:
                print("âœ… AI Ä‘Ã£ tá»•ng há»£p thÃ nh cÃ´ng")
                final_content = synthesized_content
                method = "ai_synthesis"
            else:
                print("âš ï¸ AI khÃ´ng pháº£n há»“i, sá»­ dá»¥ng tá»•ng há»£p cÆ¡ báº£n")
                final_content = cleaner.clean_raw_text(combined_content)
                method = "basic_synthesis"
                
        except Exception as e:
            print(f"âŒ Lá»—i AI: {e}")
            final_content = cleaner.clean_raw_text(combined_content)
            method = "basic_synthesis"
    else:
        print("ğŸ”§ Sá»­ dá»¥ng lÃ m sáº¡ch cÆ¡ báº£n...")
        final_content = cleaner.clean_raw_text(combined_content)
        method = "basic_synthesis"
    
    # BÆ°á»›c 4: Táº¡o bÃ i viáº¿t wiki cuá»‘i cÃ¹ng
    print("\nğŸ“š BÆ¯á»šC 4: Táº¡o bÃ i viáº¿t wiki")
    print("-" * 50)
    
    # Táº¡o pháº§n nguá»“n tham kháº£o
    sources_section = "\n## Nguá»“n tham kháº£o\n\n"
    for i, source in enumerate(sources, 1):
        sources_section += f"{i}. [{source['title']}]({source['url']})\n"
    
    # Táº¡o ná»™i dung markdown hoÃ n chá»‰nh
    wiki_content = f"""# {topic}

> **TÃ³m táº¯t:** BÃ i viáº¿t tá»•ng há»£p tá»« {len(sources)} nguá»“n tin uy tÃ­n vá» {topic.lower()}.

{final_content}

{sources_section}

---

**PhÆ°Æ¡ng phÃ¡p:** {method}  
**Sá»‘ nguá»“n:** {len(sources)} trang web  
**Thá»i gian táº¡o:** {time.strftime('%d/%m/%Y %H:%M')}  
**CÃ´ng cá»¥:** WikinongSang Crawler
"""
    
    # LÆ°u vÃ o thÆ° má»¥c pages
    pages_dir = Path("pages")
    pages_dir.mkdir(exist_ok=True)
    
    # Táº¡o tÃªn file an toÃ n
    safe_filename = topic.lower()
    safe_filename = ''.join(c for c in safe_filename if c.isalnum() or c in (' ', '-', '_'))
    safe_filename = safe_filename.replace(' ', '_')
    safe_filename = f"{safe_filename}_{int(time.time())}.md"
    
    wiki_file = pages_dir / safe_filename
    
    with open(wiki_file, 'w', encoding='utf-8') as f:
        f.write(wiki_content)
    
    print(f"âœ… ÄÃ£ táº¡o bÃ i viáº¿t wiki: {wiki_file}")
    
    # BÆ°á»›c 5: TÃ³m táº¯t káº¿t quáº£
    print("\nğŸ‰ HOÃ€N THÃ€NH!")
    print("=" * 60)
    print(f"ğŸ“„ BÃ i viáº¿t: {topic}")
    print(f"ğŸ“ File: {wiki_file}")
    print(f"ğŸ“Š Äá»™ dÃ i: {len(wiki_content)} kÃ½ tá»±")
    print(f"ğŸŒ Nguá»“n: {len(sources)} trang web")
    print(f"ğŸ¤– PhÆ°Æ¡ng phÃ¡p: {method}")
    print()
    print("ğŸ“‹ CÃ¡c bÆ°á»›c tiáº¿p theo:")
    print("1. Cháº¡y website: python app.py")
    print("2. Truy cáº­p: http://localhost:8000")
    print(f"3. Xem bÃ i viáº¿t: http://localhost:8000/page/{wiki_file.stem}")
    print("4. ÄÄƒng nháº­p admin Ä‘á»ƒ chá»‰nh sá»­a: /admin")
    print()
    print("ğŸ”„ Äá»ƒ thu tháº­p thÃªm bÃ i viáº¿t khÃ¡c: python crawler.py")

def single_url_crawler():
    """Thu tháº­p ná»™i dung tá»« 1 URL Ä‘Æ¡n láº»"""
    print("\nğŸ“¡ THU THáº¬P ÄÆ N Láºº")
    print("-" * 30)
    
    crawler = WebCrawler()
    cleaner = TextCleaner()
    
    url = input("Nháº­p URL: ").strip()
    if not url or not (url.startswith('http://') or url.startswith('https://')):
        print("âŒ URL khÃ´ng há»£p lá»‡!")
        return
    
    print(f"\nğŸ”„ Äang crawl: {url}")
    
    try:
        result = crawler.crawl_url(url)
        if result:
            print(f"âœ… ThÃ nh cÃ´ng: {result['title']}")
            
            # LÆ°u raw
            raw_file = crawler.save_raw_content(result)
            
            # LÃ m sáº¡ch
            cleaned_file = cleaner.process_file(str(raw_file))
            
            # Copy vÃ o pages
            pages_dir = Path("pages")
            pages_dir.mkdir(exist_ok=True)
            
            with open(cleaned_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            wiki_file = pages_dir / f"single_{int(time.time())}.md"
            with open(wiki_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"ğŸ“š ÄÃ£ táº¡o wiki: {wiki_file}")
        else:
            print("âŒ KhÃ´ng thá»ƒ crawl URL nÃ y")
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

def show_guide():
    """Hiá»ƒn thá»‹ hÆ°á»›ng dáº«n"""
    print("\nğŸ“– HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG")
    print("-" * 40)
    print()
    print("ğŸ¯ Má»¥c Ä‘Ã­ch:")
    print("- Thu tháº­p ná»™i dung tháº­t tá»« cÃ¡c trang bÃ¡o/blog")
    print("- Tá»•ng há»£p nhiá»u nguá»“n thÃ nh 1 bÃ i viáº¿t wiki")
    print("- LÃ m sáº¡ch vÃ  tá»‘i Æ°u báº±ng AI")
    print()
    print("ğŸ“‹ Quy trÃ¬nh:")
    print("1. Nháº­p 2-5 URL tá»« cÃ¡c trang tin tá»©c")
    print("2. Crawler tá»± Ä‘á»™ng thu tháº­p ná»™i dung")
    print("3. AI tá»•ng há»£p vÃ  lÃ m sáº¡ch")
    print("4. Táº¡o bÃ i viáº¿t wiki hoÃ n chá»‰nh")
    print()
    print("ğŸ’¡ Gá»£i Ã½ URL tá»‘t:")
    print("- VnExpress: https://vnexpress.net/...")
    print("- DÃ¢n TrÃ­: https://dantri.com.vn/...")
    print("- NÃ´ng nghiá»‡p VN: https://nongnghiep.vn/...")
    print("- BÃ¡o NÃ´ng thÃ´n: https://baonongthon.com.vn/...")
    print()
    print("âš ï¸ LÆ°u Ã½:")
    print("- Chá»n cÃ¡c bÃ i viáº¿t cÃ¹ng chá»§ Ä‘á»")
    print("- TrÃ¡nh trang cÃ³ quÃ¡ nhiá»u quáº£ng cÃ¡o")
    print("- Äáº£m báº£o Ollama Ä‘ang cháº¡y Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t nháº¥t")

def main():
    """ChÆ°Æ¡ng trÃ¬nh chÃ­nh vá»›i menu"""
    print("ğŸŒ¾ === WIKINONGSANG CRAWLER ===")
    print()
    print("Chá»n cháº¿ Ä‘á»™:")
    print("1. Thu tháº­p tá»« nhiá»u URL vÃ  tá»•ng há»£p thÃ nh 1 bÃ i viáº¿t")
    print("2. Thu tháº­p tá»« 1 URL Ä‘Æ¡n láº»")
    print("3. Xem hÆ°á»›ng dáº«n sá»­ dá»¥ng")
    
    choice = input("\nChá»n (1/2/3): ").strip()
    
    if choice == "1":
        multi_source_crawler()
    elif choice == "2":
        single_url_crawler()
    elif choice == "3":
        show_guide()
    else:
        print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡!")

if __name__ == "__main__":
    main()